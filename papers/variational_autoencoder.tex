\subsection{Variational autoencoder}
Assume that the data $\mathbb{X}$ are generated by some random process, involving and unobserved continuous variable $\boldsymbol{z}$, which will be referenced as a latent variable or code. The objective is once again to find the PDF of the given data in a parametric form $p_{\bt}\left(\bx\right)$. One can choose approximative distribution in the form of
\begin{equation}
p_{\bt}\left(\bx\right) = \int p_{\bt}\left(\bx,  \boldsymbol{z}\right)\d{\boldsymbol{z}} =\int p_{\bt}\left(\bx\vert \boldsymbol{z}\right)p_{\bt}\left(\boldsymbol{z}\right)\d{\boldsymbol{z}},
\end{equation}
but such approximation is very expensive to compute or can be even intractable. Intractability of the $p_{\bt}\left(\bx\right)$ makes posterior PDF $p_{\bt}\left(\boldsymbol{z}\vert\bx\right)$ also intractable. To solve this issue it is necessary to introduce further approximative posterior distribution $q_{\bphi}\left(\boldsymbol{z}\vert\bx\right) \approx p_{\bt}\left(\boldsymbol{z}\vert\bx\right)$ with parameters $\bphi$. Standard terminology refers to the model $q_{\bphi}\left(\boldsymbol{z}\vert\bx\right)$ as a probabilistic encoder and  $p_{\bt}\left(\bx\vert \boldsymbol{z}\right)$ is called a probabilistic decoder.
For variational autoencoder (VAE) the idea is to use KL distance from $q_{\bphi}\left(\boldsymbol{z}\vert\bx\right)$ to $p_{\bt}\left(\bx\vert \boldsymbol{z}\right)$, yielding
\begin{equation}\label{eq:VAEloss}
\begin{split}
D_{\mathrm{KL}}\left(q_{\bphi}\left(\boldsymbol{z}\vert \bx \right) \Vert p_{\bt}\left(\boldsymbol{z}\vert \bx\right)\right) & = 
\int q_{\bphi}\left(\boldsymbol{z}\vert \bx \right) \log \frac{q_{\bphi}\left(\boldsymbol{z}\vert \bx \right)}{p_{\bt}\left(\boldsymbol{z}\vert \bx\right)} \d{\boldsymbol{z}} \\
& =  \int q_{\bphi}\left(\boldsymbol{z}\vert \bx \right) \log \frac{q_{\bphi}\left(\boldsymbol{z}\vert \bx \right)p_{\bt}\left(\bx\right)}{p_{\bt}\left(\bx \vert \boldsymbol{z}\right) p_{\bt}\left(\boldsymbol{z}\right)} \d{\boldsymbol{z}} \\
& = \log p_{\bt}\left(\boldsymbol{x}\right) +  \int q_{\bphi}\left(\boldsymbol{z}\vert \bx \right) \log \frac{q_{\bphi}\left(\boldsymbol{z}\vert \bx \right)}{p_{\bt}\left(\bx \vert \boldsymbol{z}\right)p_{\bt}\left(\boldsymbol{z}\right) } \d{\boldsymbol{z}} \\
& = \log p_{\bt}\left(\boldsymbol{x}\right) +  \mathbb{E}_{q_{\bphi}\left(\boldsymbol{z}\vert \bx \right)}\left[\log\frac{q_{\bphi}\left(\boldsymbol{z}\vert \bx \right)}{p_{\bt}\left(\boldsymbol{z}\right)} - \log p\left(\textbf{x}\vert \boldsymbol{z}\right)\right]\\
    & = \log p_{\bt}\left(\boldsymbol{x}\right) +\KL{q_{\bphi}\left(\boldsymbol{z}\vert \bx \right)}{p_{\bt}\left(\boldsymbol{z}\right)} -  \mathbb{E}_{q_{\bphi}\left(\boldsymbol{z}\vert \bx \right)}\left[\log p\left(\bx\vert \boldsymbol{z}\right)\right].
 \end{split}
\end{equation}
Using the last equality of \eqref{eq:VAEloss}, it is possible to rewrite the equation into its typical form
\begin{equation}
\log p_{\bt}\left(\bx\right) - D_{\mathrm{KL}}\left(q_{\bphi}\left(\boldsymbol{z}\vert \bx \right) \Vert p_{\bt}(\boldsymbol{z}\vert \bx)\right) = \mathbb{E}_{q_{\bphi}\left(\boldsymbol{z}\vert \bx \right)}\left[\log p_{\bt}(\bx\vert \boldsymbol{z}) \right] - D_{\mathrm{KL}}\left(q_{\bphi}\left(\boldsymbol{z}\vert\bx \right) \Vert p_{\bt}\left(\boldsymbol{z}\right)\right),
\end{equation}
where the right hand side is called evidence lower bound (ELBO). It means that 
Jestliže vybereme parametrickou formu distribuce
\begin{equation}
q_{\bphi}\left(\boldsymbol{z}\vert \bx \right) = \pazocal{N}\left(\mu_{\phi}(\bx), \diag\left(\sigma^2_{\phi}(\bx)\right) \right) ,
\end{equation}
můžeme parametry $\theta$ a $\phi$ minimalizovat zároveň a to následovně
\begin{equation}
\begin{split}
\hat{\theta},\hat{\phi} & = \argmin_{\theta, \phi}\sum_{i=1}^n\log p\left(x_i\right) \\
& = \argmin_{\theta, \phi}\left\lbrace  \mathbb{E}_q\left[\log p(\textbf{x}\vert \textbf{z}) \right] - D_{KL}\left(q\left(\textbf{z}\vert \textbf{x} \right) \Vert p(\textbf{z})\right)\right\rbrace . 
\end{split}
\end{equation}

\begin{equation}
\bx = f_{\bt}(\boldsymbol{z}) + \epsilon ,
\end{equation}
kde $\epsilon \sim \pazocal{N}\left(0,\sigma^2\cdot\mathbb{I}   \right)$ and  $f_{\theta}(\textbf{z})$ je neuronová síť.  Využijeme následující formu aproximace

Podle vztahu pro $\textbf{x}$ určíme distribuce $p\left(\textbf{x}\vert \textbf{z}\right)$ a $p\left(\textbf{z}\right)$ zvolíme jednoduše
\begin{equation}\label{VAE_latent}
\begin{split}
 p(\textbf{x}\vert \textbf{z}) &= \pazocal{N}\left(f_{\theta}(\textbf{z}),\sigma^2\cdot\mathbb{I} \right), \\
p(\textbf{z}) &= \pazocal{N}\left(0,\mathbb{I} \right).
\end{split}
\end{equation}

\subsubsection{Naivní přístup}
K nalezení $p(\textbf{x})$ je třeba najít parametry $\theta$ transformace $f_{\theta}(\textbf{z})$, proto zkusme sestavit věrohodnostní funkci $ \log p\left( \textbf{x}\right) = \log \prod_{i = 1}^n p\left(x_i \right) $  a minimalizovat 
\begin{equation}
\begin{split}
\hat{\theta} & = \argmin_{\theta} \sum_{i=1}^n \log p\left(x_{i} \right)\\
& =  \argmin_{\theta} \sum_{i=1}^n \log \int \pazocal{N}\left(f_{\theta}(z_j),\sigma^2 \right)\cdot\pazocal{N}\left(0,1 \right)    \dd z_j \\
& = \argmin_{\theta} \sum_{i=1}^n \ \log\sum_{j=1}^n \exp \left\lbrace -\frac{1}{2\sigma^2} \left(x_i - f_{\theta}(z_j)  \right)^2 \right\rbrace \cdot \exp \left\lbrace -\frac{z_j^2}{2} \right\rbrace .
\end{split}
\end{equation}
Integrace přes $\textbf{z}$ je nahrazena vzorkováním. Tento postup ovšem při minimalizaci nemusí konvergovat ke správným výsledkům.
\subsection{Variační Bayesova metoda}
Lepší metodou se ukazuje vzorkovat z podmíněné distribuce $q(\textbf{z}\vert \textbf{x})$ a využít ELBO

\begin{equation}
\begin{split}
D_{KL}\left(q\left(\textbf{z}\vert \textbf{x} \right) \Vert p(\textbf{z}\vert \textbf{x})\right) & = 
 \mathbb{E}_q\left[\log q(\textbf{z}\vert \textbf{x}) - \log p\left(\textbf{z}\vert \textbf{x} \right)\right] \\
 & =  \mathbb{E}_q\left[\log q(\textbf{z}\vert \textbf{x}) - \log p(\textbf{x}\vert \textbf{z}) - \log p(\textbf{z}) + \log p(\textbf{x})   \right] .
 \end{split}
\end{equation}
Tuto rovnici můžeme přepsat pomocí KL--divergence 
\begin{equation}
\log p(\textbf{x}) - D_{KL}\left(q\left(\textbf{z}\vert \textbf{x} \right) \Vert p(\textbf{z}\vert \textbf{x})\right) = \mathbb{E}_q\left[\log p(\textbf{x}\vert \textbf{z}) \right] - D_{KL}\left(q\left(\textbf{z}\vert\textbf{x} \right) \Vert p(\textbf{z})\right) ,
\end{equation}
kde pravá strana této rovnice je lower bound objektu $\log p(\textbf{x})$.
Jestliže vybereme parametrickou formu distribuce
\begin{equation}
q\left(\textbf{z}\vert \textbf{x} \right) = \pazocal{N}\left(\mu_{\phi}(\textbf{x}), \diag\left(\sigma^2_{\phi}(\textbf{x})\right) \right) ,
\end{equation}
můžeme parametry $\theta$ a $\phi$ minimalizovat zároveň a to následovně
\begin{equation}
\begin{split}
\hat{\theta},\hat{\phi} & = \argmin_{\theta, \phi}\sum_{i=1}^n\log p\left(x_i\right) \\
& = \argmin_{\theta, \phi}\left\lbrace  \mathbb{E}_q\left[\log p(\textbf{x}\vert \textbf{z}) \right] - D_{KL}\left(q\left(\textbf{z}\vert \textbf{x} \right) \Vert p(\textbf{z})\right)\right\rbrace . 
\end{split}
\end{equation}
V metodě variačního autoencoderu jsou nezbytné následující dva fakty.
\begin{enumerate}
\item Trik v \textbf{reparametrizaci}
\begin{equation}
\textbf{z} = \mu_{\phi}(\textbf{x}) + \sigma_{\phi}(\textbf{x})\odot\epsilon ,
\end{equation}
kde $\odot$ značí Hadamardův součin, čili součin po složkách.
To můžeme zapsat jednodušeji takto
\begin{equation}
z_i = \mu_{\phi}(x_i) + \sigma_{\phi}(x_i)\cdot\epsilon_i .
\end{equation}
Nejedná se v podstatě o nic jiného, než o transformaci náhodné veličiny.
\item  KL--divergence dvou Gaussovských distribucí má analytické řešení a nabude tvaru
\begin{equation}
\begin{split}
 D_{KL}\left(q\left(\textbf{z}\vert \textbf{x} \right) \Vert p(\textbf{z})\right) & = \frac{1}{2}\left[\tr\left(\diag\left(\sigma_{\phi}^2(\textbf{x})\right)\right) - \mu_{\phi}\tran(\textbf{x})\mu_{\phi}(\textbf{x}) - k - \log\det \diag\left(\sigma_{\phi}^2(\textbf{x})   \right)\right] \\
 & = \frac{1}{2}\left[\sum_{l = 1}^k(\sigma_{\phi}^2(\textbf{x})) -\mu_\phi\tran(\textbf{x})\mu_{\phi}(\textbf{x}) - k - \sum_{l = 1}^k\log\sigma_{\phi}^2(\textbf{x})    \right],
\end{split}
\end{equation}
kde $k$ značí dimenzi Gaussova rozdělení. 
\end{enumerate}

Kdybychom totiž nevybrali aproximační distribuce Gaussovské, nemohli bychom tímto způsobem $\hat{\theta},\hat{\phi}$ určit. Díky těmto dvěma faktům tak získáme konečný tvar odhadu parametrů

\begin{multline}\label{řešení_VAE}
 \hat{\theta},\hat{\phi}   = \argmin_{\theta, \phi}\sum_{i = 1}^n\sum_{j = 1}^p \left[ x_i - f_\theta \left(\mu_{\phi}(x_i) + \sigma_{\phi}(x_i)\cdot\epsilon_{i,j}  \right)        \right] ^2  \\ -   \frac{1}{2}\left[\sum_{l = 1}^k(\sigma^2_{\phi}(x_i)) -\mu_{\phi}\tran(x_i)\mu_{\phi}(x_i) - k - \sum_{l = 1}^k\log\sigma^2_{\phi}(x_i)   \right] .
\end{multline}

