%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.

\documentclass[12pt,oneside, american, intlimits]{book}
%\documentclass[oneside,intlimits,reqno]{scrbook}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=4cm,bmargin=3cm,lmargin=3cm,rmargin=2cm,headheight=0.8cm,headsep=1cm,footskip=0.5cm}
\pagestyle{headings}
\setcounter{secnumdepth}{3}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{ {figures/} }

\usepackage{setspace}
\usepackage{subfig}
\usepackage{tikz}

\usepackage{array}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{pgf,tikz}
\usetikzlibrary{arrows}
\usepackage{extarrows}
\usepackage{graphicx,makeidx}
\usepackage{a4wide}
\usepackage{ragged2e}
\usepackage[nottoc]{tocbibind}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{mathabx}
\usepackage[czech]{babel}
\usepackage{relsize}
\makeatletter

\usepackage[colorlinks]{hyperref}
\usepackage{nomencl}
\makenomenclature

\renewcommand{\nomname}{List of Symbols}

\renewcommand{\nompreamble}{The next list describes several symbols that will be later used within the body of the document}

\usepackage{acronym}





\usepackage[inline,attach]{asymptote}
\def\asydir{pictures/source}
\usepackage[dvips]{attachfile2}


\input{commands.tex}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
%\usepackage{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}
\usepackage{wrapfig}
%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}

\makeatother

\usepackage{babel}

\begin{document}



\input{setting_intro.tex}

\thispagestyle{empty}
\SkipTocEntry\listoffigures
\SkipTocEntry\listoftables
\newpage
%\pagenumbering{arabic}

\input{list_of_symbols.tex}

\input{list_of_acronyms.tex}

\chapter*{Introduction}

\addcontentsline{toc}{chapter}{Introduction}

In the field of supervised learning \cite{supervised} has been achieved a tremendous progress and success in recent years. Examples of such successes include speech recognition \cite{speech} or anomaly detection \cite{Pevnak}. A classification task is typically addressed minimizing a cross entropy loss, which is defined as an expected value of logarithm of Softmax function. \\
Contrastive learning \cite{contrastive1, contrastive2} is a machine learning method often used in representation learning for image classification or video understanding. For training such models is, most of the time, minimized the contrastive loss, which reduces the 'distance' between representations of different augmented views of the same image and increases the distance between representations of augmented views of different images. \\
In this research project, these two objectives are brought together and utilized in the form of a hybrid combination \cite{HDGEmain}, which is used instead of the aformentioned cross entropy loss. Consequently, this approach is applied on multiple instance learning problems , where is taken advantage of the unified framework HMill and Mill.jl library \cite{mandlik} implemented in Julia programming language \cite{Julia}.\\
This work is arranged into 3 chapters in a logical sequence. In the first chapter is written theoretical introduction needed for a better understanding of the whole work. The second chapter consists of discriminative and generative modeling and its hybrid combination, where the simple polynomial regression experiment is performed. In the last, third, chapter is introduced the multiple instance learning with following experiments. 
The primary goal of this work is to test out the hybrid approach on the real data and, eventually, show its benefits in comparison to discriminative learning. 

\input{theoritecal_intro.tex}

\input{discr_vs_gen.tex}

\input{hybrid_mod.tex}

\input{MIL.tex}

\chapter*{Conclusion}

\pagestyle{plain}

\addcontentsline{toc}{chapter}{Conclusion}
At the beginning of this work, supervised learning and energy-based models were introduced. The following passage consists of contrastive learning, which was briefly reviewed, and thereafter supervised learning and contrastive learning was merged into hybrid discriminative and generative models. Subsequently, this approach was applied and tested on a simple example consisting of polynomial regression.
After this example, we briefly introduced multiple instance learning with a short description of embedded space and its way of training. As a first MIL experiment, cross--validation was performed on four MIL datasets, where enough space was given to a proper definition of the model complexity metrics.  The results obtained were totally expected; they included decreasing prediction errors for train data and increasing for test data. In the next step, a solution was searched on how to decrease the prediction error evaluated on the test data. In this initiative, HDGM was trained instead of a standard discriminative model. In the consecutive result, it was found that HDGM leads to a decrease in prediction error, however, nothing significant. In addition, a high number of random splits of the data set was still necessary to eliminate the noise from the prediction error.
In conclusion, this approach was proven to be functional, although a proposed generative regularization is very simple and can be replaced by much complicated models. From this point of view, this approach has great potential that has not yet been fully exploited.


\input{reference.tex}
\appendix
\chapter{Computional formulas}
\input{appendix.tex}
\end{document}
