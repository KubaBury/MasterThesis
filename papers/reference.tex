\begin{thebibliography}{1}

\bibitem{bishop}Christopher M.Bishop: \emph{Pattern recognition and machine learning}. [New York]: Springer, 2006. Information science and statistics. ISBN 0-387-31073-8.

\bibitem{mandlik}S.Mandlik.: \emph{Mapping the Internet: Modelling Entity
	Interactions in Complex Heterogeneous Networks}. arXiv preprint arXiv:2104.09650.

\bibitem{Pevnak}T. Pevny and P. Somol: 	\emph{Discriminative models for multi-instance problems with tree structure.} In Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security, 2016, 83-91.

\bibitem{MIL}J. Wu, S. Pan, X. Zhu, C. Zhang, X. Wu: \emph{Multi-instance learning with discriminative bag mapping.} IEEE Transactions on Knowledge and Data Engineering, 30(6), 2018, 1065-1080.

\bibitem{jeffrey}H. Jeffrey: \emph{An invariant form for the prior probability in estimation problems.} Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences. 1946, 1--9.

\bibitem{generativevsdisriminative} M. I. Jordan and A. Y. Ng. :\emph{On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes.} Advances in neural information processing systems.  2002.

\bibitem{KL}D. Commenges: \emph{Information Theory and Statistics: an overview.} ArXiv preprint arXiv:1511.00860, 2015,  1--22.

\bibitem{Bigp}J. Brownlee: \emph{How to Handle Big-p, Little-n (p >> n) in Machine Learning}. (2020). [on-line]. Available from: \url{https://machinelearningmastery.com/how-to-handle-big-p-little-n-p-n-in-machine-learning/}.

\bibitem{smidl} V. Smidl: \emph{The Variational Bayes Approach in Signal procesing}. PhD Thesis. Trinity College Dublin. 2004.

\bibitem{contrastive1} M. Zhuang and M. Collins: \emph{Ma, Zhuang, and Michael Collins. "Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency.} arXiv preprint arXiv:1809.01812, 2018.

\bibitem{contrastive2} M. Gutmann and A. Hyvärinen: \emph{Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.}  Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010.

\bibitem{HDGEmain} H. Liu and P. Abbeel: \emph{Hybrid discriminative-generative training via contrastive learning.} arXiv preprint arXiv:2007.09070, 2020.

\bibitem{EM} S.K. Ng, T. Krishnan and G.J.McLachlan: \emph{Handbook of computional statistics}. The EM algorithm. Springer, Berlin, Heidelberg. 2012, 139-172.

\bibitem{energy} W. Gratwohl, K.C. Wang and J.H. Jacobsen: \emph{Your classifier is secretly an energy based model and you should treat it like one.} GRATHWOHL, Will, et al. Your classifier is secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019.

\bibitem{energy2} Y. LeCun, S. Chopra, R.Hadsell, M. Ranzato and F. Huang: \emph{A tutorial on energy-based learning.}  Predicting structured data, 2006, 1.0.

\bibitem{MILfirstly} T.G. Dietterich, R.H. Lathrop and T. Lozano-Pérez: \emph{Solving the multiple instance problem with axis-parallel rectangles.}  Artificial intelligence, 1997, 89.1-2: 31-71.

\bibitem{statistics} J. Friedman, T. Hastie and R. Tibshirani: \emph{The elements of statistical learning.} [New York]: Springer, 2001. Series in statistics.

\bibitem{supervised} M. Talabis, R. McPherson, I. Miyamoto, J. Martin and D.Kaye: \emph{Information Security Analytics.} Syngress, 2015.

\bibitem{Julia} J.Bezanson, S. Karpinski, V.B. Skah:\emph{A fast dynamic language for technical computing.} arXiv preprint arXiv:1209.5145, 2012.

\bibitem{speech} D. Yu and L. Deng: \emph{Automatic Speech Recognition.} Springer london limited, 2016.
\end{thebibliography}
